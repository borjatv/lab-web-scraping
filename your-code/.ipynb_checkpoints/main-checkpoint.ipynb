{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "import random\n",
    "import re\n",
    "# import scrapy\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comento esta celda porque si no ocuparía muchísimo en el notebook\n",
    "\n",
    "# print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "developers = soup.findAll('h1', attrs = {'class':\"h3 lh-condensed\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n            Earle F. Philhower, III\\n '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "developers[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Earle F. Philhower, III',\n",
       " 'Jan De Dobbeleer',\n",
       " 'Artem Zakharchenko',\n",
       " 'David Tolnay',\n",
       " 'Fabian Affolter',\n",
       " 'Luke Edwards',\n",
       " 'Dirkjan Ochtman',\n",
       " 'Rich Harris',\n",
       " 'Michael[tm] Smith',\n",
       " 'Michael Lingelbach',\n",
       " 'Claudéric Demers',\n",
       " 'Robert Mosolgo',\n",
       " 'Hans-Kristian Arntzen',\n",
       " '/rootzoll',\n",
       " 'Remi Rousselet',\n",
       " 'Daniel Wirtz',\n",
       " 'Florian Märkl',\n",
       " 'andig',\n",
       " 'Mladen Macanović',\n",
       " 'Razzeee',\n",
       " 'Aimeos',\n",
       " 'Lipis',\n",
       " 'Josh Bleecher Snyder',\n",
       " 'nukeop',\n",
       " 'Ben Johnson']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "developers_list = [re.findall('\\\\n\\\\n\\s*(.*)\\\\n', a.text)[0]  for a in developers]\n",
    "developers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "resp =  requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = soup.findAll('h1', attrs = {'class':\"h3 lh-condensed\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['public-apis/public-apis',\n",
       " 'davidbombal/red-python-scripts',\n",
       " 'lucidrains/deep-daze',\n",
       " 'davepl/Primes',\n",
       " 'sebastianruder/NLP-progress',\n",
       " 'TheAlgorithms/Python',\n",
       " 'donnemartin/system-design-primer',\n",
       " 'ticarpi/jwt_tool',\n",
       " '3b1b/manim',\n",
       " 'freqtrade/freqtrade-strategies',\n",
       " 'sqlmapproject/sqlmap',\n",
       " 'swisskyrepo/PayloadsAllTheThings',\n",
       " 'danpaquin/coinbasepro-python',\n",
       " 'spotDL/spotify-downloader',\n",
       " 'intel-isl/DPT',\n",
       " 'freqtrade/freqtrade',\n",
       " 'searx/searx',\n",
       " 'kholia/OSX-KVM',\n",
       " 'ReFirmLabs/binwalk',\n",
       " 'gonzaarcr/Fildem',\n",
       " 'SystemErrorWang/White-box-Cartoonization',\n",
       " 'monosans/vk-slaves-bot',\n",
       " 'quantopian/zipline',\n",
       " 'sympy/sympy',\n",
       " 'ytdl-org/youtube-dl']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos_list = [a.text.replace('\\n', '').replace(' ','') for a in repos]\n",
    "repos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "resp = requests.get(url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pics = soup.findAll('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<img alt=\"This is a featured article. Click here for more information.\" data-file-height=\"438\" data-file-width=\"462\" decoding=\"async\" height=\"19\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/30px-Cscr-featured.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/40px-Cscr-featured.svg.png 2x\" width=\"20\"/>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pics_list = [a.get('src') for a in pics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/footer/wikimedia-button.png',\n",
       " '/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "resp = requests.get(url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacelinks = soup.find('div', attrs={'id':'bodyContent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Python/wiki/Pythons',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/CMU_Common_Lisp',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/PERQ#PERQ_3',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(painter)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_of_Byzantium',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_of_Catana',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_Anghelo',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(Ford_prototype)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(nuclear_primary)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Colt_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/PYTHON',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(film)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Monty_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(Monty)_Pictures',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Cython',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Pyton',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Pithon']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos la lista de páginas sobre Python en la Wikipedia\n",
    "listpages = [f\"{url}\"+spacelinks.findAll('li')[x].find('a').get('href') for x in range(len(spacelinks.findAll('li')))\n",
    "if not (spacelinks.findAll('li')[x].find('a').get('href').startswith('#')\n",
    "        or spacelinks.findAll('li')[x].find('a').get('href').startswith('/wiki/Category:'))]\n",
    "listpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entramos en una de las páginas anteriores al azar\n",
    "resp2 = requests.get(listpages[random.randint(0, len(listpages))])\n",
    "soup2 = BeautifulSoup(resp2.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wiktionary.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikibooks.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikiquote.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikisource.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikiversity.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://commons.wikimedia.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikivoyage.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikinews.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://www.wikidata.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://species.wikimedia.org/wiki/Special:Search/Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/w/index.php?search=Python%2Fwiki%2FPython+%28mythology%29&title=Special%3ASearch&fulltext=1',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Python%2Fwiki%2FPython+%28mythology%29',\n",
       " 'https://en.wikipedia.org/w/index.php?search=Python%2Fwiki%2FPython+%28mythology%29&title=Special%3ASearch&fulltext=1&ns0=1',\n",
       " 'https://en.wikipedia.org/w/index.php?search=Python%2Fwiki%2FPython+%28mythology%29&title=Special%3ASearch&fulltext=1',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:Log/delete&page=Python/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/wiki/Python/wiki/Python_(mythology)',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sacamos la lista de enlaces que empiecen por 'http'\n",
    "links = []\n",
    "for a in soup2.findAll():\n",
    "    try:\n",
    "        if a.get('href').startswith('http'):\n",
    "            links.append(a.get('href'))\n",
    "    except:\n",
    "        pass\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "resp = requests.get(url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.findAll('div', attrs = {\"class\":\"usctitlechanged\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Titles that have changed in the United States Code since its last release point: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Titles that have changed in the United States Code since its last release point: {len(titles)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "resp = requests.get(url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbi = soup.findAll('h3', attrs = {'class':'title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'EUGENE PALMER',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbi_list = [a.text.replace('\\n','') for a in fbi]\n",
    "fbi_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11 abril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "resp = requests.get(url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd = soup.findAll('tr', attrs = {'class':'ligne1 normal'})\n",
    "even = soup.findAll('tr', attrs = {'class':'ligne2 normal'})\n",
    "todas = odd + even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_dict = [{'date': re.findall('\\d{4}\\-\\d{2}\\-\\d{2}', x.text)[0],\n",
    "  'time': re.findall('\\d{2}\\:\\d{2}\\:\\d{2}', x.text)[0],\n",
    "  'latitude': x.findAll('td', attrs={'class': 'tabev1'})[0].text[:-1] + ' ' + x.findAll('td', attrs={'class': 'tabev2'})[0].text[:-2],\n",
    "    'longitude': x.findAll('td', attrs={'class': 'tabev1'})[1].text[:-1] + ' ' + x.findAll('td', attrs={'class': 'tabev2'})[1].text[:-2],\n",
    "    'region_name': x.find('td', attrs={'class':'tb_region'}).text[1:]\n",
    "} for x in todas   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(earthquakes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20 = df.sort_values('datetime', ascending=False).head(20).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region_name</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-11</td>\n",
       "      <td>00:17:13</td>\n",
       "      <td>38.27 N</td>\n",
       "      <td>20.71 E</td>\n",
       "      <td>GREECE</td>\n",
       "      <td>2021-04-11 00:17:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-11</td>\n",
       "      <td>00:12:30</td>\n",
       "      <td>8.72 N</td>\n",
       "      <td>84.12 W</td>\n",
       "      <td>OFF COAST OF COSTA RICA</td>\n",
       "      <td>2021-04-11 00:12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-11</td>\n",
       "      <td>00:04:20</td>\n",
       "      <td>19.25 N</td>\n",
       "      <td>155.42 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2021-04-11 00:04:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-11</td>\n",
       "      <td>00:03:07</td>\n",
       "      <td>23.09 S</td>\n",
       "      <td>68.75 W</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>2021-04-11 00:03:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>23:54:49</td>\n",
       "      <td>8.32 S</td>\n",
       "      <td>112.32 E</td>\n",
       "      <td>JAVA, INDONESIA</td>\n",
       "      <td>2021-04-10 23:54:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>23:47:28</td>\n",
       "      <td>24.18 S</td>\n",
       "      <td>67.01 W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "      <td>2021-04-10 23:47:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>23:33:15</td>\n",
       "      <td>34.42 S</td>\n",
       "      <td>179.15 E</td>\n",
       "      <td>SOUTH OF KERMADEC ISLANDS</td>\n",
       "      <td>2021-04-10 23:33:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>23:22:30</td>\n",
       "      <td>18.43 N</td>\n",
       "      <td>69.29 W</td>\n",
       "      <td>DOMINICAN REPUBLIC REGION</td>\n",
       "      <td>2021-04-10 23:22:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>23:15:38</td>\n",
       "      <td>38.28 N</td>\n",
       "      <td>20.73 E</td>\n",
       "      <td>GREECE</td>\n",
       "      <td>2021-04-10 23:15:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>23:00:21</td>\n",
       "      <td>29.15 N</td>\n",
       "      <td>129.39 E</td>\n",
       "      <td>RYUKYU ISLANDS, JAPAN</td>\n",
       "      <td>2021-04-10 23:00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>22:32:51</td>\n",
       "      <td>18.48 N</td>\n",
       "      <td>101.58 W</td>\n",
       "      <td>GUERRERO, MEXICO</td>\n",
       "      <td>2021-04-10 22:32:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>22:24:45</td>\n",
       "      <td>36.90 N</td>\n",
       "      <td>10.93 W</td>\n",
       "      <td>AZORES-CAPE ST. VINCENT RIDGE</td>\n",
       "      <td>2021-04-10 22:24:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>22:24:39</td>\n",
       "      <td>37.38 S</td>\n",
       "      <td>179.94 E</td>\n",
       "      <td>OFF E. COAST OF N. ISLAND, N.Z.</td>\n",
       "      <td>2021-04-10 22:24:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>22:20:25</td>\n",
       "      <td>30.44 S</td>\n",
       "      <td>71.65 W</td>\n",
       "      <td>COQUIMBO, CHILE</td>\n",
       "      <td>2021-04-10 22:20:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>22:07:37</td>\n",
       "      <td>36.03 N</td>\n",
       "      <td>98.54 W</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>2021-04-10 22:07:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>22:07:21</td>\n",
       "      <td>36.48 N</td>\n",
       "      <td>27.16 E</td>\n",
       "      <td>DODECANESE IS.-TURKEY BORDER REG</td>\n",
       "      <td>2021-04-10 22:07:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>21:55:25</td>\n",
       "      <td>20.48 S</td>\n",
       "      <td>69.11 W</td>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "      <td>2021-04-10 21:55:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>21:55:05</td>\n",
       "      <td>1.02 N</td>\n",
       "      <td>126.87 E</td>\n",
       "      <td>MOLUCCA SEA</td>\n",
       "      <td>2021-04-10 21:55:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>21:49:37</td>\n",
       "      <td>28.25 N</td>\n",
       "      <td>67.34 E</td>\n",
       "      <td>PAKISTAN</td>\n",
       "      <td>2021-04-10 21:49:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>21:35:28</td>\n",
       "      <td>66.35 N</td>\n",
       "      <td>157.29 W</td>\n",
       "      <td>NORTHERN ALASKA</td>\n",
       "      <td>2021-04-10 21:35:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date      time latitude longitude                       region_name  \\\n",
       "0   2021-04-11  00:17:13  38.27 N   20.71 E                            GREECE   \n",
       "1   2021-04-11  00:12:30   8.72 N   84.12 W           OFF COAST OF COSTA RICA   \n",
       "2   2021-04-11  00:04:20  19.25 N  155.42 W          ISLAND OF HAWAII, HAWAII   \n",
       "3   2021-04-11  00:03:07  23.09 S   68.75 W                ANTOFAGASTA, CHILE   \n",
       "4   2021-04-10  23:54:49   8.32 S  112.32 E                   JAVA, INDONESIA   \n",
       "5   2021-04-10  23:47:28  24.18 S   67.01 W                  SALTA, ARGENTINA   \n",
       "6   2021-04-10  23:33:15  34.42 S  179.15 E         SOUTH OF KERMADEC ISLANDS   \n",
       "7   2021-04-10  23:22:30  18.43 N   69.29 W         DOMINICAN REPUBLIC REGION   \n",
       "8   2021-04-10  23:15:38  38.28 N   20.73 E                            GREECE   \n",
       "9   2021-04-10  23:00:21  29.15 N  129.39 E             RYUKYU ISLANDS, JAPAN   \n",
       "10  2021-04-10  22:32:51  18.48 N  101.58 W                  GUERRERO, MEXICO   \n",
       "11  2021-04-10  22:24:45  36.90 N   10.93 W     AZORES-CAPE ST. VINCENT RIDGE   \n",
       "12  2021-04-10  22:24:39  37.38 S  179.94 E   OFF E. COAST OF N. ISLAND, N.Z.   \n",
       "13  2021-04-10  22:20:25  30.44 S   71.65 W                   COQUIMBO, CHILE   \n",
       "14  2021-04-10  22:07:37  36.03 N   98.54 W                          OKLAHOMA   \n",
       "15  2021-04-10  22:07:21  36.48 N   27.16 E  DODECANESE IS.-TURKEY BORDER REG   \n",
       "16  2021-04-10  21:55:25  20.48 S   69.11 W                   TARAPACA, CHILE   \n",
       "17  2021-04-10  21:55:05   1.02 N  126.87 E                       MOLUCCA SEA   \n",
       "18  2021-04-10  21:49:37  28.25 N   67.34 E                          PAKISTAN   \n",
       "19  2021-04-10  21:35:28  66.35 N  157.29 W                   NORTHERN ALASKA   \n",
       "\n",
       "              datetime  \n",
       "0  2021-04-11 00:17:13  \n",
       "1  2021-04-11 00:12:30  \n",
       "2  2021-04-11 00:04:20  \n",
       "3  2021-04-11 00:03:07  \n",
       "4  2021-04-10 23:54:49  \n",
       "5  2021-04-10 23:47:28  \n",
       "6  2021-04-10 23:33:15  \n",
       "7  2021-04-10 23:22:30  \n",
       "8  2021-04-10 23:15:38  \n",
       "9  2021-04-10 23:00:21  \n",
       "10 2021-04-10 22:32:51  \n",
       "11 2021-04-10 22:24:45  \n",
       "12 2021-04-10 22:24:39  \n",
       "13 2021-04-10 22:20:25  \n",
       "14 2021-04-10 22:07:37  \n",
       "15 2021-04-10 22:07:21  \n",
       "16 2021-04-10 21:55:25  \n",
       "17 2021-04-10 21:55:05  \n",
       "18 2021-04-10 21:49:37  \n",
       "19 2021-04-10 21:35:28  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 29 marzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "resp = requests.get(url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd = soup.findAll('tr', attrs = {'class':'ligne1 normal'})\n",
    "even = soup.findAll('tr', attrs = {'class':'ligne2 normal'})\n",
    "todas = odd + even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_dict = [{'date': re.findall('\\d{4}\\-\\d{2}\\-\\d{2}', x.text)[0],\n",
    "  'time': re.findall('\\d{2}\\:\\d{2}\\:\\d{2}', x.text)[0],\n",
    "  'latitude': x.findAll('td', attrs={'class': 'tabev1'})[0].text[:-1] + ' ' + x.findAll('td', attrs={'class': 'tabev2'})[0].text[:-2],\n",
    "    'longitude': x.findAll('td', attrs={'class': 'tabev1'})[1].text[:-1] + ' ' + x.findAll('td', attrs={'class': 'tabev2'})[1].text[:-2],\n",
    "    'region_name': x.find('td', attrs={'class':'tb_region'}).text[1:]\n",
    "} for x in todas   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(earthquakes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20 = df.sort_values('datetime', ascending=False).head(20).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region_name</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>14:38:22</td>\n",
       "      <td>15.28 N</td>\n",
       "      <td>91.78 W</td>\n",
       "      <td>GUATEMALA</td>\n",
       "      <td>2021-03-29 14:38:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>14:31:41</td>\n",
       "      <td>19.15 N</td>\n",
       "      <td>155.49 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2021-03-29 14:31:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>13:57:26</td>\n",
       "      <td>39.84 N</td>\n",
       "      <td>21.92 E</td>\n",
       "      <td>GREECE</td>\n",
       "      <td>2021-03-29 13:57:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>13:42:58</td>\n",
       "      <td>16.08 N</td>\n",
       "      <td>91.35 W</td>\n",
       "      <td>CHIAPAS, MEXICO</td>\n",
       "      <td>2021-03-29 13:42:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>13:35:26</td>\n",
       "      <td>7.01 N</td>\n",
       "      <td>126.74 E</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "      <td>2021-03-29 13:35:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>13:22:51</td>\n",
       "      <td>19.47 N</td>\n",
       "      <td>155.65 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2021-03-29 13:22:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>13:22:33</td>\n",
       "      <td>25.56 N</td>\n",
       "      <td>91.91 E</td>\n",
       "      <td>MEGHALAYA, INDIA REGION</td>\n",
       "      <td>2021-03-29 13:22:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>13:00:58</td>\n",
       "      <td>19.45 N</td>\n",
       "      <td>155.66 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2021-03-29 13:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>12:59:56</td>\n",
       "      <td>24.18 S</td>\n",
       "      <td>67.17 W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "      <td>2021-03-29 12:59:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>12:52:40</td>\n",
       "      <td>42.70 N</td>\n",
       "      <td>16.19 E</td>\n",
       "      <td>ADRIATIC SEA</td>\n",
       "      <td>2021-03-29 12:52:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>12:45:53</td>\n",
       "      <td>38.27 N</td>\n",
       "      <td>38.77 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2021-03-29 12:45:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>12:43:54</td>\n",
       "      <td>18.13 N</td>\n",
       "      <td>71.51 W</td>\n",
       "      <td>DOMINICAN REPUBLIC</td>\n",
       "      <td>2021-03-29 12:43:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>12:10:53</td>\n",
       "      <td>36.41 N</td>\n",
       "      <td>7.87 W</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "      <td>2021-03-29 12:10:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>11:53:14</td>\n",
       "      <td>28.15 S</td>\n",
       "      <td>67.20 W</td>\n",
       "      <td>CATAMARCA, ARGENTINA</td>\n",
       "      <td>2021-03-29 11:53:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>11:50:11</td>\n",
       "      <td>46.27 N</td>\n",
       "      <td>7.94 E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "      <td>2021-03-29 11:50:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>11:25:12</td>\n",
       "      <td>18.87 S</td>\n",
       "      <td>69.58 W</td>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "      <td>2021-03-29 11:25:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>11:13:46</td>\n",
       "      <td>37.20 N</td>\n",
       "      <td>3.69 W</td>\n",
       "      <td>SPAIN</td>\n",
       "      <td>2021-03-29 11:13:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>11:04:31</td>\n",
       "      <td>43.12 N</td>\n",
       "      <td>3.09 E</td>\n",
       "      <td>NEAR SOUTH COAST OF FRANCE</td>\n",
       "      <td>2021-03-29 11:04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>10:56:31</td>\n",
       "      <td>19.19 N</td>\n",
       "      <td>155.45 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2021-03-29 10:56:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>10:50:41</td>\n",
       "      <td>20.35 S</td>\n",
       "      <td>68.94 W</td>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "      <td>2021-03-29 10:50:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date      time latitude longitude                 region_name  \\\n",
       "0   2021-03-29  14:38:22  15.28 N   91.78 W                   GUATEMALA   \n",
       "1   2021-03-29  14:31:41  19.15 N  155.49 W    ISLAND OF HAWAII, HAWAII   \n",
       "2   2021-03-29  13:57:26  39.84 N   21.92 E                      GREECE   \n",
       "3   2021-03-29  13:42:58  16.08 N   91.35 W             CHIAPAS, MEXICO   \n",
       "4   2021-03-29  13:35:26   7.01 N  126.74 E       MINDANAO, PHILIPPINES   \n",
       "5   2021-03-29  13:22:51  19.47 N  155.65 W    ISLAND OF HAWAII, HAWAII   \n",
       "6   2021-03-29  13:22:33  25.56 N   91.91 E     MEGHALAYA, INDIA REGION   \n",
       "7   2021-03-29  13:00:58  19.45 N  155.66 W    ISLAND OF HAWAII, HAWAII   \n",
       "8   2021-03-29  12:59:56  24.18 S   67.17 W            SALTA, ARGENTINA   \n",
       "9   2021-03-29  12:52:40  42.70 N   16.19 E                ADRIATIC SEA   \n",
       "10  2021-03-29  12:45:53  38.27 N   38.77 E              EASTERN TURKEY   \n",
       "11  2021-03-29  12:43:54  18.13 N   71.51 W          DOMINICAN REPUBLIC   \n",
       "12  2021-03-29  12:10:53  36.41 N    7.87 W         STRAIT OF GIBRALTAR   \n",
       "13  2021-03-29  11:53:14  28.15 S   67.20 W        CATAMARCA, ARGENTINA   \n",
       "14  2021-03-29  11:50:11  46.27 N    7.94 E                 SWITZERLAND   \n",
       "15  2021-03-29  11:25:12  18.87 S   69.58 W             TARAPACA, CHILE   \n",
       "16  2021-03-29  11:13:46  37.20 N    3.69 W                       SPAIN   \n",
       "17  2021-03-29  11:04:31  43.12 N    3.09 E  NEAR SOUTH COAST OF FRANCE   \n",
       "18  2021-03-29  10:56:31  19.19 N  155.45 W    ISLAND OF HAWAII, HAWAII   \n",
       "19  2021-03-29  10:50:41  20.35 S   68.94 W             TARAPACA, CHILE   \n",
       "\n",
       "              datetime  \n",
       "0  2021-03-29 14:38:22  \n",
       "1  2021-03-29 14:31:41  \n",
       "2  2021-03-29 13:57:26  \n",
       "3  2021-03-29 13:42:58  \n",
       "4  2021-03-29 13:35:26  \n",
       "5  2021-03-29 13:22:51  \n",
       "6  2021-03-29 13:22:33  \n",
       "7  2021-03-29 13:00:58  \n",
       "8  2021-03-29 12:59:56  \n",
       "9  2021-03-29 12:52:40  \n",
       "10 2021-03-29 12:45:53  \n",
       "11 2021-03-29 12:43:54  \n",
       "12 2021-03-29 12:10:53  \n",
       "13 2021-03-29 11:53:14  \n",
       "14 2021-03-29 11:50:11  \n",
       "15 2021-03-29 11:25:12  \n",
       "16 2021-03-29 11:13:46  \n",
       "17 2021-03-29 11:04:31  \n",
       "18 2021-03-29 10:56:31  \n",
       "19 2021-03-29 10:50:41  "
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<strong>English</strong>,\n",
       " <strong>EspaÃ±ol</strong>,\n",
       " <strong>æ¥æ¬èª</strong>,\n",
       " <strong>Deutsch</strong>,\n",
       " <strong>Ð ÑÑÑÐºÐ¸Ð¹</strong>,\n",
       " <strong>FranÃ§ais</strong>,\n",
       " <strong>Italiano</strong>,\n",
       " <strong>ä¸­æ</strong>,\n",
       " <strong>PortuguÃªs</strong>,\n",
       " <strong>Polski</strong>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('strong', attrs={'class': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " 'EspaÃ±ol',\n",
       " 'æ\\x97¥æ\\x9c¬èª\\x9e',\n",
       " 'Deutsch',\n",
       " 'Ð\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹',\n",
       " 'FranÃ§ais',\n",
       " 'Italiano',\n",
       " 'ä¸\\xadæ\\x96\\x87',\n",
       " 'PortuguÃªs',\n",
       " 'Polski']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_languages = [a.text for a in soup.findAll('strong', attrs={'class': ''})]\n",
    "list_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_data = [a.text for a in soup.findAll('h3', attrs={'class': 'govuk-heading-s dgu-topics__heading'})]\n",
    "uk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "# Sacamos todas las tablas de la página con pd.read_html(url)\n",
    "tables = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>Percentageof world pop.(March 2019)[8]</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918.0</td>\n",
       "      <td>11.922%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480.0</td>\n",
       "      <td>5.994%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379.0</td>\n",
       "      <td>4.922%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank          Language  Speakers(millions)  \\\n",
       "0     1  Mandarin Chinese               918.0   \n",
       "1     2           Spanish               480.0   \n",
       "2     3           English               379.0   \n",
       "\n",
       "  Percentageof world pop.(March 2019)[8] Language family    Branch  \n",
       "0                                11.922%    Sino-Tibetan   Sinitic  \n",
       "1                                 5.994%   Indo-European   Romance  \n",
       "2                                 4.922%   Indo-European  Germanic  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos interesa la 2ª tabla\n",
    "tables[1].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>Percentageof world pop.(March 2019)[8]</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918.0</td>\n",
       "      <td>11.922%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480.0</td>\n",
       "      <td>5.994%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379.0</td>\n",
       "      <td>4.922%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (sanskritised Hindustani)[9]</td>\n",
       "      <td>341.0</td>\n",
       "      <td>4.429%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>228.0</td>\n",
       "      <td>2.961%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221.0</td>\n",
       "      <td>2.870%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.662%</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi[10]</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                            Language  Speakers(millions)  \\\n",
       "0     1                    Mandarin Chinese               918.0   \n",
       "1     2                             Spanish               480.0   \n",
       "2     3                             English               379.0   \n",
       "3     4  Hindi (sanskritised Hindustani)[9]               341.0   \n",
       "4     5                             Bengali               228.0   \n",
       "5     6                          Portuguese               221.0   \n",
       "6     7                             Russian               154.0   \n",
       "7     8                            Japanese               128.0   \n",
       "8     9                 Western Punjabi[10]                92.7   \n",
       "9    10                             Marathi                83.1   \n",
       "\n",
       "  Percentageof world pop.(March 2019)[8] Language family        Branch  \n",
       "0                                11.922%    Sino-Tibetan       Sinitic  \n",
       "1                                 5.994%   Indo-European       Romance  \n",
       "2                                 4.922%   Indo-European      Germanic  \n",
       "3                                 4.429%   Indo-European    Indo-Aryan  \n",
       "4                                 2.961%   Indo-European    Indo-Aryan  \n",
       "5                                 2.870%   Indo-European       Romance  \n",
       "6                                 2.000%   Indo-European  Balto-Slavic  \n",
       "7                                 1.662%         Japonic      Japanese  \n",
       "8                                 1.204%   Indo-European    Indo-Aryan  \n",
       "9                                 1.079%   Indo-European    Indo-Aryan  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_languages = tables[1].sort_values('Rank').head(10)\n",
    "top_10_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El padrino'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Movie name\n",
    "soup.findAll('td', attrs={'class': 'titleColumn'})[1].find('a').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1972"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial release\n",
    "int(soup.findAll('td', attrs={'class': 'titleColumn'})[1].find('span').text.replace('(','').replace(')',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frank Darabont'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Director\n",
    "re.findall('(.*) \\(dir\\.\\)', soup.findAll('td', attrs={'class': 'titleColumn'})[0].find('a').get_attribute_list('title')[0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stars\n",
    "float(soup.findAll('td', attrs={'class': 'ratingColumn imdbRating'})[1].find('strong').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dict = [{'name': soup.findAll('td', attrs={'class': 'titleColumn'})[x].find('a').text,\n",
    "'release': int(soup.findAll('td', attrs={'class': 'titleColumn'})[x].find('span').text.replace('(','').replace(')','')),\n",
    "'director': re.findall('(.*) \\(dir\\.\\)', soup.findAll('td', attrs={'class': 'titleColumn'})[x].find('a').get_attribute_list('title')[0])[0],\n",
    "'stars': float(soup.findAll('td', attrs={'class': 'ratingColumn imdbRating'})[x].find('strong').text)\n",
    "}for x in range(250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.DataFrame(movies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>release</th>\n",
       "      <th>director</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  release              director  stars\n",
       "0        Cadena perpetua     1994        Frank Darabont    9.2\n",
       "1             El padrino     1972  Francis Ford Coppola    9.1\n",
       "2   El padrino: Parte II     1974  Francis Ford Coppola    9.0\n",
       "3    El caballero oscuro     2008     Christopher Nolan    9.0\n",
       "4  12 hombres sin piedad     1957          Sidney Lumet    8.9"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/title/tt0111161/'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Link\n",
    "soup.findAll('td', attrs={'class': 'titleColumn'})[0].find('a').get_attribute_list('href')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un nuevo df con el top 10 de pelis\n",
    "\n",
    "moviestop10_dict = [{'name': soup.findAll('td', attrs={'class': 'titleColumn'})[x].find('a').text,\n",
    "'year': int(soup.findAll('td', attrs={'class': 'titleColumn'})[x].find('span').text.replace('(','').replace(')','')),\n",
    "'link': soup.findAll('td', attrs={'class': 'titleColumn'})[x].find('a').get_attribute_list('href')[0]\n",
    "}for x in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = pd.DataFrame(moviestop10_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>/title/tt0111161/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>/title/tt0068646/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>/title/tt0071562/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>/title/tt0468569/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>/title/tt0050083/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>La lista de Schindler</td>\n",
       "      <td>1993</td>\n",
       "      <td>/title/tt0108052/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El señor de los anillos: El retorno del rey</td>\n",
       "      <td>2003</td>\n",
       "      <td>/title/tt0167260/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>1994</td>\n",
       "      <td>/title/tt0110912/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>El bueno, el feo y el malo</td>\n",
       "      <td>1966</td>\n",
       "      <td>/title/tt0060196/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>El señor de los anillos: La comunidad del anillo</td>\n",
       "      <td>2001</td>\n",
       "      <td>/title/tt0120737/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  year               link\n",
       "0                                   Cadena perpetua  1994  /title/tt0111161/\n",
       "1                                        El padrino  1972  /title/tt0068646/\n",
       "2                              El padrino: Parte II  1974  /title/tt0071562/\n",
       "3                               El caballero oscuro  2008  /title/tt0468569/\n",
       "4                             12 hombres sin piedad  1957  /title/tt0050083/\n",
       "5                             La lista de Schindler  1993  /title/tt0108052/\n",
       "6       El señor de los anillos: El retorno del rey  2003  /title/tt0167260/\n",
       "7                                      Pulp Fiction  1994  /title/tt0110912/\n",
       "8                        El bueno, el feo y el malo  1966  /title/tt0060196/\n",
       "9  El señor de los anillos: La comunidad del anillo  2001  /title/tt0120737/"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteramos a través del df para añadir la descripción haciendo request de cada link\n",
    "\n",
    "for i,c in top10.iterrows():\n",
    "    soup2 = BeautifulSoup(requests.get(f\"https://www.imdb.com{c['link']}\").text, 'html.parser')\n",
    "    a = soup2.find('div', attrs={\"class\":\"summary_text\"}).text.replace('\\n','')\n",
    "    top10.loc[i, 'description'] = re.sub('^(\\s*)|(\\s*)$','',a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando la columna 'link' ya tendríamos el df deseado\n",
    "\n",
    "top10.drop('link', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>An organized crime dynasty's aging patriarch t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>The early life and career of Vito Corleone in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>A jury holdout attempts to prevent a miscarria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>La lista de Schindler</td>\n",
       "      <td>1993</td>\n",
       "      <td>In German-occupied Poland during World War II,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El señor de los anillos: El retorno del rey</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gandalf and Aragorn lead the World of Men agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>1994</td>\n",
       "      <td>The lives of two mob hitmen, a boxer, a gangst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>El bueno, el feo y el malo</td>\n",
       "      <td>1966</td>\n",
       "      <td>A bounty hunting scam joins two men in an unea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>El señor de los anillos: La comunidad del anillo</td>\n",
       "      <td>2001</td>\n",
       "      <td>A meek Hobbit from the Shire and eight compani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  year  \\\n",
       "0                                   Cadena perpetua  1994   \n",
       "1                                        El padrino  1972   \n",
       "2                              El padrino: Parte II  1974   \n",
       "3                               El caballero oscuro  2008   \n",
       "4                             12 hombres sin piedad  1957   \n",
       "5                             La lista de Schindler  1993   \n",
       "6       El señor de los anillos: El retorno del rey  2003   \n",
       "7                                      Pulp Fiction  1994   \n",
       "8                        El bueno, el feo y el malo  1966   \n",
       "9  El señor de los anillos: La comunidad del anillo  2001   \n",
       "\n",
       "                                         description  \n",
       "0  Two imprisoned men bond over a number of years...  \n",
       "1  An organized crime dynasty's aging patriarch t...  \n",
       "2  The early life and career of Vito Corleone in ...  \n",
       "3  When the menace known as the Joker wreaks havo...  \n",
       "4  A jury holdout attempts to prevent a miscarria...  \n",
       "5  In German-occupied Poland during World War II,...  \n",
       "6  Gandalf and Aragorn lead the World of Men agai...  \n",
       "7  The lives of two mob hitmen, a boxer, a gangst...  \n",
       "8  A bounty hunting scam joins two men in an unea...  \n",
       "9  A meek Hobbit from the Shire and eight compani...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_report():\n",
    "    \"\"\"\n",
    "    Returns a weather report of a given city in a pandas DataFrame format.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = 'http://api.openweathermap.org/data/2.5/weather'\n",
    "    \n",
    "    languages_list = ['pt_br','zh_cn','zh_tw','af','al','ar','az','bg','ca','cz','da','de','el','en','eu','fa','fi',\n",
    "    'fr','gl','he','hi','hr','hu','id','it','ja','kr','la','lt','mk','no','nl','pl','pt','ro','ru','sv','se','sk',\n",
    "    'sl','sp','es','sr','th','tr','ua','uk','vi','zu']\n",
    "    \n",
    "    filename = '../../../tokens/openweathermap.txt'\n",
    "    with open(filename, 'r') as f:\n",
    "        token = f.read()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            cityname = input('Enter the city: ')\n",
    "            if not requests.get(url, params = {'q': cityname, 'appid': token}).status_code == 200:\n",
    "                raise ValueError\n",
    "            break\n",
    "        except:\n",
    "            print('The city is not available. Try again.')\n",
    " \n",
    "    while True:\n",
    "        try:\n",
    "            lang = input('Enter the language in 2 digit code for the description: ')\n",
    "            if lang not in languages_list:\n",
    "                raise ValueError\n",
    "            break\n",
    "        except:\n",
    "            print(f'The language code is not available. Try again with one of the following languages \\n {languages_list}')\n",
    "    \n",
    "    \n",
    "    resp = requests.get(url, params = {'q': cityname, 'appid': token, 'lang': lang})\n",
    "        \n",
    "    return pd.DataFrame([{\n",
    "        'city': resp.json()['name'],\n",
    "        'temp_celsius': round(resp.json()['main']['temp']-273, 1),\n",
    "        'wind_speed': resp.json()['wind']['speed'],\n",
    "        'humidity': resp.json()['main']['humidity'],\n",
    "        'main': resp.json()['weather'][0]['main'],\n",
    "        'description': resp.json()['weather'][0]['description']\n",
    "        \n",
    "    }]).set_index('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city: Fake city\n",
      "The city is not available. Try again.\n",
      "Enter the city: Pontevedra\n",
      "Enter the language in 2 digit code for the description: Fake language\n",
      "The language code is not available. Try again with one of the following languages \n",
      " ['pt_br', 'zh_cn', 'zh_tw', 'af', 'al', 'ar', 'az', 'bg', 'ca', 'cz', 'da', 'de', 'el', 'en', 'eu', 'fa', 'fi', 'fr', 'gl', 'he', 'hi', 'hr', 'hu', 'id', 'it', 'ja', 'kr', 'la', 'lt', 'mk', 'no', 'nl', 'pl', 'pt', 'ro', 'ru', 'sv', 'se', 'sk', 'sl', 'sp', 'es', 'sr', 'th', 'tr', 'ua', 'uk', 'vi', 'zu']\n",
      "Enter the language in 2 digit code for the description: gl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_celsius</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>humidity</th>\n",
       "      <th>main</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pontevedra</th>\n",
       "      <td>13.8</td>\n",
       "      <td>6.17</td>\n",
       "      <td>54</td>\n",
       "      <td>Clear</td>\n",
       "      <td>ceo claro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            temp_celsius  wind_speed  humidity   main description\n",
       "city                                                             \n",
       "Pontevedra          13.8        6.17        54  Clear   ceo claro"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3 = BeautifulSoup(requests.get(f\"{url}catalogue/page-1.html\").text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "books =  soup3.findAll('article', attrs={'class': 'product_pod'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Light in the Attic'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#book - tenemos que sacarlo de 'img' porque en \n",
    "books[0].find('h3').find('a').get('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.77"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#price\n",
    "float(books[0].find('p', attrs={'class': 'price_color'}).text[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'            In stock    '"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stock availability\n",
    "books[0].find('p', attrs={'class': 'instock availability'}).text.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In stock'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('^(\\s*)|(\\s*)$','',books[0].find('p', attrs={'class': 'instock availability'}).text.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_dict = []\n",
    "for n in range(int(1000/20)):\n",
    "    soup3 = BeautifulSoup(requests.get(f\"{url}catalogue/page-{n}.html\").text, 'html.parser')\n",
    "    books =  soup3.findAll('article', attrs={'class': 'product_pod'})\n",
    "    for x in books:\n",
    "        books_dict.append({'name': x.find('h3').find('a').get('title'),\n",
    "                        'price': float(x.find('p', attrs={'class': 'price_color'}).text[2:]),\n",
    "                        'stock_availability': re.sub('^(\\s*)|(\\s*)$','',books[0].find('p', attrs={'class': 'instock availability'}).text.replace('\\n',''))})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame(books_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 3)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>stock_availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  price stock_availability\n",
       "0                   A Light in the Attic  51.77           In stock\n",
       "1                     Tipping the Velvet  53.74           In stock\n",
       "2                             Soumission  50.10           In stock\n",
       "3                          Sharp Objects  47.82           In stock\n",
       "4  Sapiens: A Brief History of Humankind  54.23           In stock"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "278.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
